{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using LSTM to detect churn in the twitter dataset\n",
    "## ALso using word-embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords=stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silverstar/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers import concatenate,Flatten,Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import RNN\n",
    "from keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainDF=pd.read_csv(\"dataV2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands=np.unique(mainDF['brand'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gone mangy cat got go  <bb>  drop cable box remote gamestop exchange game rack screws'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mainDF.iloc()[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter=PorterStemmer()\n",
    "def preprocessing(a_tweet):\n",
    "    cleanTweet=[]\n",
    "    a_tweet=a_tweet.split(\" \")\n",
    "    for word in a_tweet:\n",
    "        word=word.strip()\n",
    "        valid=re.search(pattern=r\"[^A-Za-z<>]\",string=word)==None\n",
    "        if(word!=\"\" and valid==True and word not in stopwords):\n",
    "            word=porter.stem(word)\n",
    "            cleanTweet.append(word)\n",
    "    return \" \".join(cleanTweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainDF['text']=mainDF['text'].apply(preprocessing)\n",
    "\n",
    "mainDF=mainDF.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gone mangi cat got go <bb> drop cabl box remot gamestop exchang game rack screw'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mainDF.iloc()[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3919x4333 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 29181 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countVect=CountVectorizer()\n",
    "countVect.fit_transform(mainDF['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=countVect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertWord(tweet):\n",
    "    tweet=tweet.split(\" \")\n",
    "    indexTweet=[]\n",
    "    for word in tweet:\n",
    "        word=word.lower()\n",
    "        if word in vocab:\n",
    "            indexTweet.append(vocab.index(word))\n",
    "    return indexTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainDF['text']=mainDF['text'].apply(convertWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tweet_length=-1\n",
    "max_tweet_index=0\n",
    "for tweet in mainDF['text']:\n",
    "    if(len(tweet)>max_tweet_length):\n",
    "        max_tweet_length=len(tweet)\n",
    "        max_tweet_index=tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainDF['brand']=LabelEncoder().fit_transform(mainDF['brand'])\n",
    "mainDF['label']=LabelEncoder().fit_transform(mainDF['choose_one'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=sequence.pad_sequences(mainDF['text'].ravel(),max_tweet_length)\n",
    "y=mainDF['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y.ravel(),stratify=y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words=len(vocab)\n",
    "embedding_vecor_length = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedding(top_words, embedding_vecor_length, input_length=max_tweet_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 22, 32)            138656    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 3)                 432       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 139,092\n",
      "Trainable params: 139,092\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "top_words=len(vocab)\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_tweet_length))\n",
    "model.add(LSTM(3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2939 samples, validate on 980 samples\n",
      "Epoch 1/30\n",
      "2939/2939 [==============================] - 0s 123us/step - loss: 0.2610 - acc: 0.9619 - val_loss: 0.6255 - val_acc: 0.7541\n",
      "Epoch 2/30\n",
      "2939/2939 [==============================] - 0s 109us/step - loss: 0.2450 - acc: 0.9622 - val_loss: 0.6336 - val_acc: 0.7582\n",
      "Epoch 3/30\n",
      "2939/2939 [==============================] - 0s 148us/step - loss: 0.2411 - acc: 0.9639 - val_loss: 0.6486 - val_acc: 0.7520\n",
      "Epoch 4/30\n",
      "2939/2939 [==============================] - 1s 172us/step - loss: 0.2280 - acc: 0.9629 - val_loss: 0.6543 - val_acc: 0.7551\n",
      "Epoch 5/30\n",
      "2939/2939 [==============================] - 0s 140us/step - loss: 0.2257 - acc: 0.9643 - val_loss: 0.6553 - val_acc: 0.7541\n",
      "Epoch 6/30\n",
      "2939/2939 [==============================] - 0s 114us/step - loss: 0.2199 - acc: 0.9633 - val_loss: 0.6622 - val_acc: 0.7541\n",
      "Epoch 7/30\n",
      "2939/2939 [==============================] - 0s 122us/step - loss: 0.2509 - acc: 0.9524 - val_loss: 0.7397 - val_acc: 0.7173\n",
      "Epoch 8/30\n",
      "2939/2939 [==============================] - 0s 111us/step - loss: 0.2563 - acc: 0.9435 - val_loss: 0.7013 - val_acc: 0.7286\n",
      "Epoch 9/30\n",
      "2939/2939 [==============================] - 0s 115us/step - loss: 0.2270 - acc: 0.9575 - val_loss: 0.6640 - val_acc: 0.7520\n",
      "Epoch 10/30\n",
      "2939/2939 [==============================] - 0s 116us/step - loss: 0.2170 - acc: 0.9619 - val_loss: 0.7023 - val_acc: 0.7347\n",
      "Epoch 11/30\n",
      "2939/2939 [==============================] - 0s 112us/step - loss: 0.2066 - acc: 0.9660 - val_loss: 0.6718 - val_acc: 0.7531\n",
      "Epoch 12/30\n",
      "2939/2939 [==============================] - 0s 110us/step - loss: 0.1971 - acc: 0.9663 - val_loss: 0.6651 - val_acc: 0.7592\n",
      "Epoch 13/30\n",
      "2939/2939 [==============================] - 0s 113us/step - loss: 0.1889 - acc: 0.9714 - val_loss: 0.6858 - val_acc: 0.7520\n",
      "Epoch 14/30\n",
      "2939/2939 [==============================] - 0s 116us/step - loss: 0.1832 - acc: 0.9711 - val_loss: 0.6829 - val_acc: 0.7551\n",
      "Epoch 15/30\n",
      "2939/2939 [==============================] - 0s 129us/step - loss: 0.1786 - acc: 0.9718 - val_loss: 0.6922 - val_acc: 0.7541\n",
      "Epoch 16/30\n",
      "2939/2939 [==============================] - 0s 148us/step - loss: 0.1748 - acc: 0.9728 - val_loss: 0.7005 - val_acc: 0.7551\n",
      "Epoch 17/30\n",
      "2939/2939 [==============================] - 1s 176us/step - loss: 0.1704 - acc: 0.9721 - val_loss: 0.7019 - val_acc: 0.7551\n",
      "Epoch 18/30\n",
      "2939/2939 [==============================] - 0s 125us/step - loss: 0.1679 - acc: 0.9738 - val_loss: 0.7154 - val_acc: 0.7541\n",
      "Epoch 19/30\n",
      "2939/2939 [==============================] - 0s 122us/step - loss: 0.1670 - acc: 0.9697 - val_loss: 0.7008 - val_acc: 0.7592\n",
      "Epoch 20/30\n",
      "2939/2939 [==============================] - 0s 119us/step - loss: 0.1652 - acc: 0.9724 - val_loss: 0.7319 - val_acc: 0.7541\n",
      "Epoch 21/30\n",
      "2939/2939 [==============================] - 0s 122us/step - loss: 0.1674 - acc: 0.9724 - val_loss: 0.7416 - val_acc: 0.7510\n",
      "Epoch 22/30\n",
      "2939/2939 [==============================] - 0s 126us/step - loss: 0.1565 - acc: 0.9728 - val_loss: 0.7329 - val_acc: 0.7541\n",
      "Epoch 23/30\n",
      "2939/2939 [==============================] - 0s 124us/step - loss: 0.1532 - acc: 0.9735 - val_loss: 0.7401 - val_acc: 0.7541\n",
      "Epoch 24/30\n",
      "2939/2939 [==============================] - 0s 123us/step - loss: 0.1501 - acc: 0.9758 - val_loss: 0.7470 - val_acc: 0.7531\n",
      "Epoch 25/30\n",
      "2939/2939 [==============================] - 0s 117us/step - loss: 0.1476 - acc: 0.9752 - val_loss: 0.7469 - val_acc: 0.7582\n",
      "Epoch 26/30\n",
      "2939/2939 [==============================] - 0s 132us/step - loss: 0.1451 - acc: 0.9752 - val_loss: 0.7696 - val_acc: 0.7480\n",
      "Epoch 27/30\n",
      "2939/2939 [==============================] - 0s 140us/step - loss: 0.1452 - acc: 0.9735 - val_loss: 0.7663 - val_acc: 0.7520\n",
      "Epoch 28/30\n",
      "2939/2939 [==============================] - 0s 122us/step - loss: 0.1422 - acc: 0.9769 - val_loss: 0.7725 - val_acc: 0.7520\n",
      "Epoch 29/30\n",
      "2939/2939 [==============================] - 0s 120us/step - loss: 0.1407 - acc: 0.9752 - val_loss: 0.7732 - val_acc: 0.7510\n",
      "Epoch 30/30\n",
      "2939/2939 [==============================] - 0s 119us/step - loss: 0.1390 - acc: 0.9745 - val_loss: 0.7949 - val_acc: 0.7469\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f151db91b00>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30,batch_size=128,class_weight={0:6,1:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSummary(y_test,y_pred):\n",
    "    class0List=np.where(y_test==0)\n",
    "    class1List=np.where(y_test==1)\n",
    "    sumDict={}\n",
    "    sumDict['accuracy']=np.mean(y_test==y_pred)\n",
    "    sumDict['class0Accuracy']=np.mean(y_test[class0List]==y_pred[class0List])\n",
    "    sumDict['class1Accuracy']=np.mean(y_test[class1List]==y_pred[class1List])\n",
    "    sumDict['precision']=precision_score(y_test,y_pred)\n",
    "    sumDict['recall']=recall_score(y_test,y_pred)\n",
    "    sumDict['f1-score']=f1_score(y_test,y_pred)\n",
    "    sumDict['auc']=roc_auc_score(y_test,y_pred)\n",
    "    sumDict['confuseMatrix']=confusion_matrix(y_test,y_pred)\n",
    "    print(classification_report(y_test,y_pred,target_names=['class 0','class 1']))\n",
    "#     print(sumDict)\n",
    "    return sumDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.42      0.51      0.46       207\n",
      "     class 1       0.86      0.81      0.83       773\n",
      "\n",
      "   micro avg       0.75      0.75      0.75       980\n",
      "   macro avg       0.64      0.66      0.65       980\n",
      "weighted avg       0.77      0.75      0.76       980\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6396730528946273,\n",
       " 'class0Accuracy': 0.5120772946859904,\n",
       " 'class1Accuracy': 0.8098318240620958,\n",
       " 'precision': 0.8610729023383769,\n",
       " 'recall': 0.8098318240620958,\n",
       " 'f1-score': 0.8346666666666666,\n",
       " 'auc': 0.6609545593740431,\n",
       " 'confuseMatrix': array([[106, 101],\n",
       "        [147, 626]])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "createSummary(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands={}\n",
    "brands['t-mobile']=['t-mobile','tmobile','tmobi']\n",
    "brands['verizon']=['verizon','veryzon']\n",
    "brands['at&t']=['att','at&t',]\n",
    "\n",
    "def generatePrediction(tweet,brand):\n",
    "    tweet=tweet.replace(\"&amp;\",\"&\")\n",
    "    tweet=tweet.replace(\"amp;\",'&')\n",
    "    for key,value in brands.items():\n",
    "        if(key==brand):\n",
    "            replaceWord=\" <bb> \"\n",
    "        else:\n",
    "            replaceWord=\" <aa> \"\n",
    "            \n",
    "        for name in value:\n",
    "            tweet=tweet.replace(name,replaceWord)\n",
    "    tweet=preprocessing(tweet)\n",
    "    tweet=convertWord(tweet)\n",
    "    X=sequence.pad_sequences([tweet],max_tweet_length)\n",
    "    outcome=model.predict_classes(X)\n",
    "    if(outcome==1):\n",
    "        return \"Not A Churn\"\n",
    "    else:\n",
    "        return \"Churn\"\n",
    "    return outcome\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Churn'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generatePrediction(\"Tired of Verizon, I hate it. I am leaving it\",\"Verizon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.75962645]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3676]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convertWord(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
